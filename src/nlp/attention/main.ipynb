{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 torch.Size([2, 3, 4])\n",
      "x2 torch.Size([2, 5, 4])\n",
      "x2转置 torch.Size([2, 4, 5])\n",
      "attn_weights torch.Size([2, 3, 5])\n",
      "attn_output torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# 点积注意力\n",
    "x1 = torch.randn(2,3,4)\n",
    "x2 = torch.randn(2,5,4)\n",
    "print(\"x1\", x1.shape)\n",
    "print(\"x2\", x2.shape)\n",
    "print(\"x2转置\", x2.transpose(1,2).shape)\n",
    "\n",
    "raw_weights = torch.bmm(x1, x2.transpose(1,2))\n",
    "# 按第三个维度进行softmax，固定的第一、二维度下，和为1\n",
    "attn_weights = F.softmax(raw_weights, dim=2)\n",
    "print(\"attn_weights\", attn_weights.shape)\n",
    "attn_output = torch.bmm(attn_weights, x2)\n",
    "print(\"attn_output\", attn_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 torch.Size([2, 3, 4])\n",
      "x2 torch.Size([2, 5, 4])\n",
      "x2转置 torch.Size([2, 4, 5])\n",
      "scaling_factor 2.0\n",
      "scaled_weights torch.Size([2, 3, 5])\n",
      "attn_weights torch.Size([2, 3, 5])\n",
      "attn_output torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# 缩放注意力\n",
    "x1 = torch.randn(2,3,4)\n",
    "x2 = torch.randn(2,5,4)\n",
    "print(\"x1\", x1.shape)\n",
    "print(\"x2\", x2.shape)\n",
    "print(\"x2转置\", x2.transpose(1,2).shape)\n",
    "\n",
    "raw_weights = torch.bmm(x1, x2.transpose(1,2))\n",
    "scaling_factor = x1.size(-1) ** 0.5  # 防梯度爆炸，按维度缩放\n",
    "print(\"scaling_factor\", scaling_factor) # 防梯度爆炸\n",
    "scaled_weights = raw_weights  / scaling_factor\n",
    "print(\"scaled_weights\", scaled_weights.shape)\n",
    "# 按第三个维度进行softmax，固定的第一、二维度下，和为1\n",
    "attn_weights = F.softmax(scaled_weights, dim=2)\n",
    "print(\"attn_weights\", attn_weights.shape)\n",
    "attn_output = torch.bmm(attn_weights, x2)\n",
    "print(\"attn_output\", attn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 3, 8])\n"
     ]
    }
   ],
   "source": [
    "# QKV\n",
    "#1. 创建 Query、Key 和 Value 张量\n",
    "q = torch.randn(2, 3, 4) # 形状 (batch_size, seq_len1, feature_dim)\n",
    "k = torch.randn(2, 4, 4) # 形状 (batch_size, seq_len2, feature_dim)\n",
    "v = torch.randn(2, 4, 8) # 形状 (batch_size, seq_len2, feature_dim)\n",
    "# 2. 计算点积，得到原始权重，形状为 (batch_size, seq_len1, seq_len2)\n",
    "raw_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "# 3. 将原始权重进行缩放（可选），形状仍为 (batch_size, seq_len1, seq_len2)\n",
    "scaling_factor = q.size(-1) ** 0.5\n",
    "scaled_weights = raw_weights / scaling_factor\n",
    "# 4. 应用 softmax 函数，使结果的值在 0 和 1 之间，且每一行的和为 1\n",
    "attn_weights = F.softmax(scaled_weights, dim=-1) # 形状仍为 (batch_size, seq_len1, seq_len2)\n",
    "print(attn_weights.shape)\n",
    "# 5. 与 Value 相乘，得到注意力分布的加权和 , 形状为 (batch_size, seq_len1, feature_dim)\n",
    "attn_output = torch.bmm(attn_weights, v)\n",
    "print(attn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_output torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    " # 一个形状为 (batch_size, seq_len, feature_dim) 的张量 x\n",
    "x = torch.randn(2, 3, 4) # 形状 (batch_size, seq_len, feature_dim)\n",
    "# 定义线性层用于将 x 转换为 Q, K, V 向量\n",
    "linear_q = torch.nn.Linear(4, 4)\n",
    "linear_k = torch.nn.Linear(4, 4)\n",
    "linear_v = torch.nn.Linear(4, 4)\n",
    "# 通过线性层计算 Q, K, V\n",
    "Q = linear_q(x) # 形状 (batch_size, seq_len, feature_dim)\n",
    "K = linear_k(x) # 形状 (batch_size, seq_len, feature_dim)\n",
    "V = linear_v(x) # 形状 (batch_size, seq_len, feature_dim)\n",
    "# 计算 Q 和 K 的点积，作为相似度分数 , 也就是自注意力原始权重\n",
    "raw_weights = torch.bmm(Q, K.transpose(1, 2)) # 形状 (batch_size, seq_len, seq_len)\n",
    "# 将自注意力原始权重进行缩放\n",
    "scale_factor = K.size(-1) ** 0.5  # 这里是 4 ** 0.5\n",
    "scaled_weights = raw_weights / scale_factor # 形状 (batch_size, seq_len, seq_len)\n",
    "# 对缩放后的权重进行 softmax 归一化，得到注意力权重\n",
    "attn_weights = F.softmax(scaled_weights, dim=2) # 形状 (batch_size, seq_len, seq_len)\n",
    "# 将注意力权重应用于 V 向量，计算加权和，得到加权信息\n",
    "attn_outputs = torch.bmm(attn_weights, V) # 形状 (batch_size, seq_len, feature_dim)\n",
    "print(\"attn_output\", attn_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape torch.Size([2, 3, 10])\n",
      "w_q.shape torch.Size([2, 3, 30])\n",
      "w_q.shape.view torch.Size([2, 3, 5, 6])\n",
      "Q.shape torch.Size([2, 5, 3, 6])\n",
      "K.shape torch.Size([2, 5, 3, 6])\n",
      "V.shape torch.Size([2, 5, 3, 6])\n",
      "attention_weights.shape torch.Size([2, 5, 3, 3])\n",
      "head_outputs.shape torch.Size([2, 5, 3, 6])\n",
      "head_outputs.transpose torch.Size([2, 3, 5, 6])\n",
      "head_outputs.transpose.contiguous torch.Size([2, 3, 5, 6])\n",
      "multi_head_output torch.Size([2, 3, 30])\n"
     ]
    }
   ],
   "source": [
    "# 假设有h个头，每个头的维度是d_k\n",
    "# 输入x的维度是(batch_size, seq_len, d_model)\n",
    "\n",
    "batch_size, seq_len, dim = 2,3,10\n",
    "h = 5\n",
    "dk = dim // h\n",
    "x = torch.randn(batch_size, seq_len, dim)\n",
    "print(\"x.shape\", x.shape)\n",
    "# 1. 线性变换生成多头的Q、K、V\n",
    "W_q = torch.nn.Linear(dim, h * d_k)\n",
    "W_k = torch.nn.Linear(dim, h * d_k)\n",
    "W_v = torch.nn.Linear(dim, h * d_k)\n",
    "\n",
    "print(\"w_q.shape\", W_q(x).shape)\n",
    "print(\"w_q.shape.view\", W_q(x).view(batch_size, seq_len, h, d_k).shape)\n",
    "\n",
    "# 2. 将输出reshape成多头形式\n",
    "Q = W_q(x).view(batch_size, seq_len, h, d_k).transpose(1, 2)\n",
    "K = W_k(x).view(batch_size, seq_len, h, d_k).transpose(1, 2)\n",
    "V = W_v(x).view(batch_size, seq_len, h, d_k).transpose(1, 2)\n",
    "print(\"Q.shape\", Q.shape)\n",
    "print(\"K.shape\", Q.shape)\n",
    "print(\"V.shape\", Q.shape)\n",
    "\n",
    "\n",
    "# 3. 每个头独立计算注意力\n",
    "attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_k**0.5)\n",
    "attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "print(\"attention_weights.shape\", attention_weights.shape)\n",
    "head_outputs = torch.matmul(attention_weights, V)\n",
    "print(\"head_outputs.shape\", head_outputs.shape)\n",
    "\n",
    "\n",
    "print(\"head_outputs.transpose\", head_outputs.transpose(1, 2).shape)\n",
    "print(\"head_outputs.transpose.contiguous\", head_outputs.transpose(1, 2).contiguous().shape)\n",
    "# print(\"head_outputs.transpose.contiguous.view\", head_outputs.transpose(1, 2).contiguous().shape)\n",
    "# 4. 合并多头的输出\n",
    "multi_head_output = head_outputs.transpose(1, 2).contiguous().view(batch_size, seq_len, h * d_k)\n",
    "print(\"multi_head_output\", multi_head_output.shape)\n",
    "\n",
    "# 5. 最后通过一个线性层整合所有头的信息\n",
    "final_output = torch.nn.Linear(h * d_k, d_model)(multi_head_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output torch.Size([2, 3, 16])\n",
      "attn_weights torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 1. 创建一个多头注意力层\n",
    "multihead_attn = torch.nn.MultiheadAttention(embed_dim=16, num_heads=8)\n",
    "# 2. 生成一个形状为 (batch_size, seq_len, feature_dim) 的张量 x\n",
    "x = torch.randn(2, 3, 16) # 形状 (batch_size, seq_len, feature_dim)\n",
    "# 3. 调用多头注意力层，得到输出和注意力权重\n",
    "output, attn_weights = multihead_attn(x, x, x)\n",
    "print(\"output\", output.shape)\n",
    "print(\"attn_weights\", attn_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
