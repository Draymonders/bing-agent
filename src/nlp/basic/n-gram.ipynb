{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-gram词频：\n",
      "('我', '喜'): {'欢': 2}\n",
      "('喜', '欢'): {'吃': 6}\n",
      "('欢', '吃'): {'苹': 2, '香': 1, '葡': 1, '⾹': 1, '草': 1}\n",
      "('吃', '苹'): {'果': 2}\n",
      "('吃', '香'): {'蕉': 1}\n",
      "('她', '喜'): {'欢': 2}\n",
      "('吃', '葡'): {'萄': 1}\n",
      "('他', '不'): {'喜': 1}\n",
      "('不', '喜'): {'欢': 1}\n",
      "('吃', '⾹'): {'蕉': 1}\n",
      "('他', '喜'): {'欢': 1}\n",
      "('吃', '草'): {'莓': 1}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "corpus = [\n",
    "    \"我喜欢吃苹果\",\n",
    "    \"我喜欢吃香蕉\",\n",
    "    \"她喜欢吃葡萄\",\n",
    "    \"他不喜欢吃⾹蕉\",\n",
    "    \"他喜欢吃苹果\",\n",
    "    \"她喜欢吃草莓\"\n",
    "]\n",
    "n = 3 # n-gram\n",
    "\n",
    "# 分词函数\n",
    "def tokenize(text):\n",
    "    return [ch for ch in text]\n",
    "\n",
    "# 计算n-grams计数\n",
    "def count_ngrams(corpus, n):\n",
    "    ngrams_count = defaultdict(Counter)\n",
    "    for text in corpus:\n",
    "        tokens = tokenize(text)\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            ngram = tuple(tokens[i:i+n])\n",
    "            prefix = ngram[:-1]\n",
    "            # print(prefix)\n",
    "            token = ngram[-1]\n",
    "            ngrams_count[prefix][token] += 1\n",
    "    return ngrams_count\n",
    "\n",
    "\n",
    "ngram_counts = count_ngrams(corpus, n)\n",
    "print(f\"{n}-gram词频：\")\n",
    "for prefix, counts in ngram_counts.items():\n",
    "    print(\"{}: {}\".format(prefix, dict(counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-gram 出现概率: \n",
      "('我', '喜'): {'欢': 1.0}\n",
      "('喜', '欢'): {'吃': 1.0}\n",
      "('欢', '吃'): {'苹': 0.3333333333333333, '香': 0.16666666666666666, '葡': 0.16666666666666666, '⾹': 0.16666666666666666, '草': 0.16666666666666666}\n",
      "('吃', '苹'): {'果': 1.0}\n",
      "('吃', '香'): {'蕉': 1.0}\n",
      "('她', '喜'): {'欢': 1.0}\n",
      "('吃', '葡'): {'萄': 1.0}\n",
      "('他', '不'): {'喜': 1.0}\n",
      "('不', '喜'): {'欢': 1.0}\n",
      "('吃', '⾹'): {'蕉': 1.0}\n",
      "('他', '喜'): {'欢': 1.0}\n",
      "('吃', '草'): {'莓': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def ngram_probabilities(ngram_counts):\n",
    "    ngram_probs = defaultdict(Counter)\n",
    "    for prefix, tokens_count in ngram_counts.items():\n",
    "        total_count = sum(tokens_count.values())\n",
    "        for token, count in tokens_count.items():\n",
    "            ngram_probs[prefix][token] = count / total_count\n",
    "    return ngram_probs\n",
    "\n",
    "ngram_probs = ngram_probabilities(ngram_counts)\n",
    "print(f\"{n}-gram 出现概率: \")\n",
    "for prefix, probs in ngram_probs.items():\n",
    "    print(f\"{prefix}: {dict(probs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_next_token(prefix, ngram_probs):\n",
    "    if not prefix in ngram_probs:\n",
    "        print(f\"{prefix} not found in ngram_probs\")\n",
    "        return None\n",
    "    next_token_probs = ngram_probs[prefix]\n",
    "    next_token = max(next_token_probs, key=next_token_probs.get) # 查找字典最大值对应的键\n",
    "    return next_token\n",
    "\n",
    "def generate_text(prefix, ngram_probs, n, length=6):\n",
    "    tokens = list(prefix)\n",
    "    for _ in range(length-len(prefix)):\n",
    "        # 获取当前前缀的下一个词\n",
    "        cur_tokens = tuple(tokens[-(n-1):])\n",
    "        print(\"当前token: {}\".format(\"\".join(cur_tokens)))\n",
    "        next_token = gen_next_token(cur_tokens, ngram_probs)\n",
    "        if not next_token:\n",
    "            break\n",
    "        print(\"下一个token:{}\".format(next_token))\n",
    "        tokens.append(next_token)\n",
    "    return \"\".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入: 我不喜欢\n",
      "当前token: 喜欢\n",
      "下一个token:吃\n",
      "当前token: 欢吃\n",
      "下一个token:苹\n",
      "当前token: 吃苹\n",
      "下一个token:果\n",
      "当前token: 苹果\n",
      "('苹', '果') not found in ngram_probs\n",
      "结果: 我不喜欢吃苹果\n"
     ]
    }
   ],
   "source": [
    "text = \"我不喜欢\"\n",
    "print(\"输入:\", text)\n",
    "text = generate_text(text, ngram_probs, n, length=10)\n",
    "print(\"结果:\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
