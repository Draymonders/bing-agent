{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "d_k = 64 # Q的维度\n",
    "d_v = 64 # V的维度\n",
    "d_embedding = 512 # embedding的维度\n",
    "n_heads = 8 # 多头注意力的个数\n",
    "batch_size = 10\n",
    "n_layers = 6 # 解码器的层数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    缩放点积注意力\n",
    "    简单理解 ScaledDotProductAttention，目的是计算Query和Key的相似权重，作用于Value\n",
    "    结果是\n",
    "    Query1: {Value1: w11, Value2: w12, Value3: w13}\n",
    "    Query2: {Value1: w21, Value2: w22, Value3: w23}\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # 维度信息\n",
    "        # Q: [batch_size, n_heads, len_q, d_k]\n",
    "        # K: [batch_size, n_heads, len_k, d_k]\n",
    "        # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "        # attn_mask: [batch_size, n_heads, len_q, len_k]\n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size, n_heads, len_q, len_k]\n",
    "        # scores: [batch_size, n_heads, len_q, len_k]\n",
    "        # 加上注意力掩码, 将attn_mask中为True的位置的分数设置为极小值\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is True.\n",
    "        # softmax归一化 => 注意力权重\n",
    "        weights = nn.Softmax(dim=-1)(scores)\n",
    "        # weights: [batch_size, n_heads, len_q, len_k]\n",
    "        context = torch.matmul(weights, V) \n",
    "        # context: [batch_size, n_heads, len_q, d_v]\n",
    "        return context, weights # 返回上下文变量 和 注意力分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    多头注意力\n",
    "    简单理解，先放大维度，提取Q、K、V的各个维度的信息，再缩小维度，得到最终的结果\n",
    "    黑盒的看是 (Q、K、V) -> Q\n",
    "    \"\"\"\n",
    "    def __init__(self, d_embedding=d_embedding, n_heads=n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_embedding = d_embedding\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(d_embedding, n_heads * d_k)\n",
    "        self.W_K = nn.Linear(d_embedding, n_heads * d_k)\n",
    "        self.W_V = nn.Linear(d_embedding, n_heads * d_v)\n",
    "        self.linear = nn.Linear(n_heads * d_v, d_embedding)\n",
    "        self.layer_norm = nn.LayerNorm(d_embedding)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # 维度信息\n",
    "        # Q: [batch_size, len_q, d_embedding]\n",
    "        # K: [batch_size, len_k, d_embedding]\n",
    "        # V: [batch_size, len_v(=len_k), d_embedding]\n",
    "        # attn_mask: [batch_size, len_q, len_k]\n",
    "        \n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # 线性层，维度提升，为了捕捉更多信息\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2) \n",
    "        # q_s: [batch_size, n_heads, len_q, d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)\n",
    "        # k_s: [batch_size, n_heads, len_k, d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)\n",
    "        # v_s: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "        # attn_mask: [batch_size, n_heads, len_q, len_k]\n",
    "\n",
    "        # 点积缩放注意力\n",
    "        context, weights = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        # context: [batch_size, n_heads, len_q, d_v]\n",
    "        # weights: [batch_size, n_heads, len_q, len_k]\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)\n",
    "        # context: [batch_size, len_q, n_heads * d_v]\n",
    "\n",
    "        # 线性层，降维成 Q 原始的维度\n",
    "        output = self.linear(context) \n",
    "        # output: [batch_size, len_q, d_embedding]\n",
    "        \n",
    "        # 残差连接，并做归一化（方便将当前Q往下层传递，所以做了残差）\n",
    "        output = self.layer_norm(output + residual) \n",
    "        # output: [batch_size, len_q, d_embedding]\n",
    "        return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    \"\"\"\n",
    "    前馈神经网络，目标是优化每个标记（单词）的表征\n",
    "    对每个位置的d_embedding维度进行升维 => 降维 => 残差归一化\n",
    "    \"\"\"\n",
    "    def __init__(self, d_ff=2048):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        # 输入升维\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_embedding, out_channels=d_ff, kernel_size=1)\n",
    "        # 输入降维\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_embedding, kernel_size=1)\n",
    "        # 定义 归一化\n",
    "        self.layer_norm = nn.LayerNorm(d_embedding)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs [batch_size, len_q, d_embedding]\n",
    "        residual = inputs\n",
    "\n",
    "        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n",
    "        # [batch_size, d_ff, len_q]\n",
    "\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        # [batch_size, len_q, d_embedding]\n",
    "        \n",
    "        output = self.layer_norm(output + residual)\n",
    "        # [batch_size, len_q, d_embedding]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_enc_table(n_position, embedding_dim):\n",
    "    # 位置编码表：目的是让模型知道输入序列中单词的位置信息\n",
    "    # 也可以用自然序列(1,2,3)作为位置编码，但正余弦能更好表达位置信息\n",
    "    # 维度信息\n",
    "    # n_position: 输入序列最大长度\n",
    "    # embedding_dim: 词向量维度\n",
    "\n",
    "    pos_table = np.zeros((n_position, embedding_dim), dtype=np.float32)\n",
    "    for pos_i in range(n_position):\n",
    "        for idx in range(embedding_dim):\n",
    "            angle = pos_i / np.power(10000, 2 * (idx // 2) / embedding_dim)\n",
    "            pos_table[pos_i, idx] = angle\n",
    "    \n",
    "    pos_table[:, 0::2] = np.sin(pos_table[:, 0::2]) # dim 2i偶数维\n",
    "    pos_table[:, 1::2] = np.cos(pos_table[:,1::2]) # dim 2i+1奇数维\n",
    "    # pos_table: [n_position, embedding_dim]\n",
    "    return torch.FloatTensor(pos_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    # 填充注意力掩码\n",
    "    # seq_q: [batch_size, len_q]\n",
    "    # seq_k: [batch_size, len_k]\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "\n",
    "    # =0的位置会变成True,其他是False\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1) \n",
    "    # [batch_size, 1, len_k]\n",
    "\n",
    "    pad_aatn_mask = pad_attn_mask.expand(batch_size, len_q, len_k)\n",
    "    # [batch_size, len_q, len_k]\n",
    "    return pad_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_subsequent_mask(seq):\n",
    "    # 注意力掩码，屏蔽未来的信息\n",
    "    # seq: [batch_size, seq_len(Q)=seq_len(K)]\n",
    "    \n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    # attn_shape: [batch_size, seq_len, seq_len]\n",
    "\n",
    "    # triu triangle upper\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
    "    # subsequent_mask: [batch_size, seq_len, seq_len]\n",
    "\n",
    "    subsequent_mask = torch.from_numpy(subsequent_mask).byte()\n",
    "    # subsequent_mask: [batch_size, seq_len, seq_len]\n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention()\n",
    "        self.feed_forward = PoswiseFeedForwardNet()\n",
    "        self.norm1 = nn.LayerNorm(d_embedding)\n",
    "        self.norm2 = nn.LayerNorm(d_embedding)\n",
    "\n",
    "    def forward(self, dec_inputs, attn_mask):\n",
    "        # dec_inputs: [batch_size, seq_len, d_embedding]\n",
    "        attn_outputs, _ = self.self_attn(dec_inputs, dec_inputs, dec_inputs, attn_mask)\n",
    "        # attn_outputs: [batch_size, seq_len, d_embedding]\n",
    "\n",
    "        # 残差连接 + 归一化\n",
    "        norm1_outputs = self.norm1(dec_inputs + attn_outputs)\n",
    "        # norm1_outputs: [batch_size, seq_len, d_embedding]\n",
    "\n",
    "        ff_outputs = self.feed_forward(norm1_outputs)\n",
    "        # ff_outputs: [batch_size, seq_len, d_embedding]\n",
    "        dec_outputs = self.norm2(norm1_outputs + ff_outputs)\n",
    "        # dec_outputs: [batch_size, seq_len, d_embedding]\n",
    "        return dec_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len):\n",
    "        super(Decoder, self).__init__()\n",
    "        # 词典维度\n",
    "        self.src_emb = nn.Embedding(vocab_size, d_embedding)\n",
    "        # 位置编码\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, d_embedding)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, dec_inputs):\n",
    "        # dec_inputs: [batch_size, seq_len]\n",
    "\n",
    "        # 创建位置编码\n",
    "        positions = torch.arange(len(dec_inputs), device=dec_inputs.device).unsqueeze(-1)\n",
    "        # positions: [batch_size, seq_len, 1]\n",
    "        inputs_embedding = self.src_emb(dec_inputs) + self.pos_emb(positions)\n",
    "        # inputs_embedding: [batch_size, seq_len, d_embedding]\n",
    "\n",
    "        # 注意力掩码，屏蔽未来的信息\n",
    "        attn_mask = get_attn_subsequent_mask(inputs_embedding).to(device)\n",
    "        attn_mask = torch.gt(attn_mask, 0)\n",
    "        # print(attn_mask.shape)\n",
    "        # print(attn_mask.dtype)\n",
    "        # attn_mask: [batch_size, seq_len, seq_len]\n",
    "\n",
    "        dec_outputs = inputs_embedding\n",
    "        for layer in self.layers:\n",
    "            dec_outputs = layer(dec_outputs, attn_mask)\n",
    "        return dec_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len):\n",
    "        super(GPT, self).__init__()\n",
    "\n",
    "        self.decoder = Decoder(vocab_size, max_seq_len) # 解码器\n",
    "        self.projection = nn.Linear(d_embedding, vocab_size) # 输出结果\n",
    "\n",
    "    def forward(self, dec_inputs):\n",
    "        dec_outputs = self.decoder(dec_inputs)\n",
    "        # dec_outputs: [batch_size, tgt_len, embedding_dim]\n",
    "        # 预测结果\n",
    "        dec_outputs = self.projection(dec_outputs)\n",
    "        # dec_outputs: [batch_size, tgt_len, vocab_size]\n",
    "        return dec_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class LanguageCorpus:\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "\n",
    "        self.seq_len = max([len(sentence.split()) for sentence in sentences]) + 2\n",
    "        self.vocab = self.create_vocab()\n",
    "        self.idx2word = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def create_vocab(self):\n",
    "        vocab = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n",
    "        word_counts = Counter()\n",
    "        for sentence in self.sentences:\n",
    "            word_counts.update(sentence.split())\n",
    "        for word in word_counts:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "        return vocab\n",
    "\n",
    "    def make_batch(self, batch_size, test_batch=False):\n",
    "        input_batch, output_batch = [], []\n",
    "        # 取batch_size个句子\n",
    "        sentence_idxs = torch.randperm(len(self.sentences))[:batch_size]\n",
    "\n",
    "        for idx in sentence_idxs:\n",
    "            sentence = self.sentences[idx]\n",
    "            # 完整seq拼接 <sos> + 句子内容 + <eos>\n",
    "            seq = [self.vocab['<sos>']] + [self.vocab[word] for word in sentence.split()] + [self.vocab['<eos>']]\n",
    "            # 序列填充到seq_len长度\n",
    "            seq += [self.vocab['<pad>']] * (self.seq_len - len(seq))\n",
    "            input_batch.append(seq[:-1])\n",
    "            output_batch.append(seq[1:])\n",
    "        return torch.LongTensor(input_batch), torch.LongTensor(output_batch)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表大小 133\n",
      "最长句子长度 17\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "with open(\"lang.txt\", \"r\") as f:\n",
    "    sentences = [line.strip() for line in f.readlines()]\n",
    "corpus = LanguageCorpus(sentences)\n",
    "vocab_size = len(corpus.vocab)\n",
    "max_seq_len = corpus.seq_len\n",
    "\n",
    "print(f\"词汇表大小 {vocab_size}\")\n",
    "print(f\"最长句子长度 {max_seq_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "epoches = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  100 cost=0.355852\n",
      "epoch:  200 cost=0.222713\n",
      "epoch:  300 cost=0.200206\n",
      "epoch:  400 cost=0.202279\n",
      "epoch:  500 cost=0.205260\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = GPT(vocab_size, max_seq_len).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    optimizer.zero_grad()\n",
    "    inputs, targets = corpus.make_batch(batch_size=batch_size)\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"epoch: {epoch+1: 04d} cost={loss:6f}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, input_str, max_len=50, debug=False):\n",
    "    model.eval()\n",
    "\n",
    "    input_tokens = [corpus.vocab[token] for token in input_str]\n",
    "    output_tokens = input_tokens.copy()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            input_tensor = torch.tensor([output_tokens]).to(device)\n",
    "            # input_tensor: [1, seq_len]\n",
    "            output = model(input_tensor)\n",
    "            # output: [1, seq_len, vocab_size]\n",
    "            next_token = torch.argmax(output[:, -1, :], dim=-1).item()\n",
    "            if next_token == corpus.vocab[\"<eos>\"]:\n",
    "                break\n",
    "            output_tokens.append(next_token)\n",
    "    output_str = \" \".join([corpus.idx2word[token] for token in output_tokens])            \n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_str:  ['I', 'am']\n",
      "gen_text:  I am learning how to build neural networks.\n",
      "input_str:  ['Python']\n",
      "gen_text:  Python is a popular programming language.\n"
     ]
    }
   ],
   "source": [
    "intput_list = [[\"I\", \"am\"], [\"Python\"]]\n",
    "\n",
    "for input_str in intput_list:\n",
    "    gen_text = generate_text(model, input_str)\n",
    "    print(\"input_str: \", input_str)\n",
    "    print(\"gen_text: \", gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17514\n",
      "2181\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_types = [\"train\", \"test\"]\n",
    "wiki_datas = {}\n",
    "\n",
    "for data_type in data_types:\n",
    "    df = pd.read_csv(f\"{data_type}.csv\")\n",
    "    print(len(df))\n",
    "    df.columns = [\"idx\", \"text\"]\n",
    "    res = []\n",
    "    for i, item in df.iterrows():\n",
    "        text = item.to_dict()[\"text\"]\n",
    "        res.append(text)\n",
    "    wiki_datas[data_type] = res\n",
    "\n",
    "def read_wikitext(data_type):\n",
    "    return wiki_datas[data_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# wiki_dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")\n",
    "# def read_wikitext(data_type, select_sentences=0):\n",
    "#     # select_sentences 选用的句子数，现在数量太大跑不动..\n",
    "#     ds = wiki_dataset[data_type]\n",
    "#     res = []\n",
    "#     idx = 0\n",
    "#     for x in ds:\n",
    "#         x = x['text'].strip()\n",
    "#         if x == \"\" or len(x) < 3:\n",
    "#             continue\n",
    "#         if x.startswith(\"=\"):\n",
    "#             continue\n",
    "#         res.append(x)\n",
    "#         idx += 1\n",
    "#         if select_sentences > 0 and idx >= select_sentences:\n",
    "#             break\n",
    "#     print(f\"wikitext: {data_type} has {len(res)} sentences\")\n",
    "#     return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iter = WikiText2(split='train') # 加载训练部分\n",
    "# wiki_data_path = \"../../../datas/wikitext-103/\"\n",
    "# def read_wikitext(file_path, ):\n",
    "#     # select_sentences 选用的句子数，现在数量太大跑不动..\n",
    "#     res = []\n",
    "#     idx = 0\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             line = line.strip()\n",
    "#             if line == \"\":\n",
    "#                 continue\n",
    "#             if line.startswith(\"=\"):\n",
    "#                 continue\n",
    "#             res.append(line)\n",
    "#             idx += 1\n",
    "#             if idx >= select_sentences:\n",
    "#                 break\n",
    "#         return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表大小 65987\n",
      "词汇示例(word2idx) [0, 1, 2, 3, 10391]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = read_wikitext(\"train\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for sentence in data_iter:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "vocab.set_default_index(vocab['<pad>'])\n",
    "\n",
    "print(\"词汇表大小\",     len(vocab))\n",
    "print(\"词汇示例(word2idx)\", vocab(['<pad>', \"<sos>\", \"<eos>\", \"the\", \"apple\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset数据条目数 17514\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 256\n",
    "\n",
    "class WikiDataset(Dataset):\n",
    "    def __init__(self, data_iter, vocab, max_len=max_seq_len):\n",
    "        self.data = []\n",
    "\n",
    "        for sentence in data_iter:\n",
    "            # 每个句子进行tokenize, 为<sos>和<eos>留空间\n",
    "            tokens = tokenizer(sentence)[:max_len-2]\n",
    "            tokens = [vocab[\"<sos>\"]] + vocab(tokens) + [vocab[\"<eos>\"]]\n",
    "            self.data.append(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source = self.data[idx][:-1]\n",
    "        target = self.data[idx][1:]\n",
    "        return torch.tensor(source), torch.tensor(target)\n",
    "\n",
    "train_dataset = WikiDataset(train_iter, vocab)\n",
    "print(\"Dataset数据条目数\", len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: torch.Size([117])\n",
      "target: torch.Size([117])\n",
      "decoded source: <sos> two manga adaptations were produced , following each of the game ' s main female protagonists imca and riela . they were senjō no valkyria 3 namo naki chikai no hana ( 戦場のヴァルキュリア3 名もなき誓いの花 , lit . valkyria of the battlefield 3 the flower of the nameless oath ) , illustrated by naoyuki fujisawa and eventually released in two volumes after being serialized in dengeki maoh between 2011 and 2012 and senjō no valkyria 3 -akaki unmei no ikusa otome- ( 戦場のヴァルキュリア3 -赤き運命の戦乙女- , lit . valkyria of the battlefield 3 -the valkyrie of the crimson fate ) , illustrated by mizuki tsuge and eventually released in a single volume by kadokawa shoten in 2012 .\n",
      "decoded target: two manga adaptations were produced , following each of the game ' s main female protagonists imca and riela . they were senjō no valkyria 3 namo naki chikai no hana ( 戦場のヴァルキュリア3 名もなき誓いの花 , lit . valkyria of the battlefield 3 the flower of the nameless oath ) , illustrated by naoyuki fujisawa and eventually released in two volumes after being serialized in dengeki maoh between 2011 and 2012 and senjō no valkyria 3 -akaki unmei no ikusa otome- ( 戦場のヴァルキュリア3 -赤き運命の戦乙女- , lit . valkyria of the battlefield 3 -the valkyrie of the crimson fate ) , illustrated by mizuki tsuge and eventually released in a single volume by kadokawa shoten in 2012 . <eos>\n"
     ]
    }
   ],
   "source": [
    "sample_source, sample_target = train_dataset[20]\n",
    "print(\"source:\", sample_source.shape)\n",
    "print(\"target:\", sample_target.shape)\n",
    "\n",
    "decoded_source = \" \".join(vocab.lookup_tokens(sample_source.tolist()))\n",
    "print(\"decoded source:\", decoded_source)\n",
    "decoded_target = \" \".join(vocab.lookup_tokens(sample_target.tolist()))\n",
    "print(\"decoded target:\", decoded_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequences, padding_value=0, length=None):\n",
    "    \"\"\"\n",
    "    填充序列，目的sequences token序列长度相同\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences) if length is None else length\n",
    "    # 全零张量\n",
    "    result = torch.full((len(sequences), max_length), padding_value, dtype=torch.long)\n",
    "\n",
    "    for i, seq in enumerate(sequences):\n",
    "        end = len(seq)\n",
    "        result[i, :end] = seq[:end]\n",
    "    return result\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    对batch数据进行预处理，让其src、tgt长度一致\n",
    "    \"\"\"\n",
    "    # batch: [(src1, tgt1), (src2, tgt2), ...]\n",
    "    sources, targets = zip(*batch)\n",
    "\n",
    "    tmps = []\n",
    "    tmps.extend(sources)\n",
    "    tmps.extend(targets)\n",
    "    max_length = max([len(s) for s in tmps])\n",
    "    pad_val = vocab[\"<pad>\"]\n",
    "\n",
    "    sources = pad_sequence(sources, padding_value=pad_val, length=max_length)\n",
    "    targets = pad_sequence(targets, padding_value=pad_val, length=max_length)\n",
    "    return sources, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_dataset 2181\n"
     ]
    }
   ],
   "source": [
    "valid_iter = read_wikitext(\"test\")\n",
    "valid_dataset = WikiDataset(valid_iter, vocab)\n",
    "print(\"valid_dataset\", len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = \"mps\"\n",
    "device = torch.device(device)\n",
    "\n",
    "model = GPT(len(vocab), max_seq_len).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "save_path = \"wikitext_best.pth\"\n",
    "epoches = 2 # 训练x轮\n",
    "min_valid_loss = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【train】epoch: 1, batch_idx: 100, loss: 0.0381\n",
      "【train】epoch: 1, batch_idx: 200, loss: 0.0214\n",
      "【train】epoch: 1, batch_idx: 300, loss: 0.0163\n",
      "【train】epoch: 1, batch_idx: 400, loss: 0.0068\n",
      "【train】epoch: 1, batch_idx: 500, loss: 0.0081\n",
      "【train】epoch: 1, batch_idx: 600, loss: 0.0060\n",
      "【train】epoch: 1, batch_idx: 700, loss: 0.0038\n",
      "【train】epoch: 1, batch_idx: 800, loss: 0.0044\n",
      "【train】epoch: 1, batch_idx: 900, loss: 0.0037\n",
      "【train】epoch: 1, batch_idx: 1000, loss: 0.0042\n",
      "【train】epoch: 1, batch_idx: 1100, loss: 0.0041\n",
      "【train】epoch: 1, batch_idx: 1200, loss: 0.0034\n",
      "【train】epoch: 1, batch_idx: 1300, loss: 0.0019\n",
      "【train】epoch: 1, batch_idx: 1400, loss: 0.0023\n",
      "【train】epoch: 1, batch_idx: 1500, loss: 0.0028\n",
      "【train】epoch: 1, batch_idx: 1600, loss: 0.0027\n",
      "【train】epoch: 1, batch_idx: 1700, loss: 0.0024\n",
      "【train】epoch: 1, loss: 0.8997923069992382\n",
      "【valid】Epoch: 1, Valid Loss: 3.7184\n",
      "【train】epoch: 2, batch_idx: 100, loss: 0.0300\n",
      "【train】epoch: 2, batch_idx: 200, loss: 0.0175\n",
      "【train】epoch: 2, batch_idx: 300, loss: 0.0133\n",
      "【train】epoch: 2, batch_idx: 400, loss: 0.0085\n",
      "【train】epoch: 2, batch_idx: 500, loss: 0.0067\n",
      "【train】epoch: 2, batch_idx: 600, loss: 0.0048\n",
      "【train】epoch: 2, batch_idx: 700, loss: 0.0043\n",
      "【train】epoch: 2, batch_idx: 800, loss: 0.0032\n",
      "【train】epoch: 2, batch_idx: 900, loss: 0.0038\n",
      "【train】epoch: 2, batch_idx: 1000, loss: 0.0026\n",
      "【train】epoch: 2, batch_idx: 1100, loss: 0.0037\n",
      "【train】epoch: 2, batch_idx: 1200, loss: 0.0023\n",
      "【train】epoch: 2, batch_idx: 1300, loss: 0.0025\n",
      "【train】epoch: 2, batch_idx: 1400, loss: 0.0027\n",
      "【train】epoch: 2, batch_idx: 1500, loss: 0.0020\n",
      "【train】epoch: 2, batch_idx: 1600, loss: 0.0022\n",
      "【train】epoch: 2, batch_idx: 1700, loss: 0.0016\n",
      "【train】epoch: 2, loss: 0.5650676170269933\n",
      "【valid】Epoch: 2, Valid Loss: 3.6216\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoches):\n",
    "    epoch_loss = 0\n",
    "    # 训练模式\n",
    "    for batch_idx, (source, target) in enumerate(train_dataloader):\n",
    "        inputs, targets = source.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs.view(-1, len(vocab)), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"【train】epoch: {epoch+1}, batch_idx: {batch_idx+1}, loss: {epoch_loss / (batch_idx + 1):.4f}\")\n",
    "        epoch_loss /= len(inputs)\n",
    "    print(f\"【train】epoch: {epoch+1}, loss: {epoch_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for source, target in valid_dataloader:\n",
    "            source = source.to(device)\n",
    "            target = target.to(device)\n",
    "            outputs = model(source)\n",
    "            loss = loss_fn(outputs.view(-1, len(vocab)), target.view(-1))\n",
    "            valid_loss += loss.item()\n",
    "    valid_loss /= len(valid_dataloader)\n",
    "    print(\"【valid】Epoch: {}, Valid Loss: {:.4f}\".format(epoch+1, valid_loss))\n",
    "    if valid_loss < min_valid_loss:\n",
    "        min_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"wikitext_best.pth\", map_location=torch.device(\"cpu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_str the first\n",
      "gen_text the first down on september . the song on the song on september 11 , the song . the song on the\n"
     ]
    }
   ],
   "source": [
    "# 集束搜索\n",
    "def print_candidate(candidate, prefix=\"\"):\n",
    "    special_tokens = {'<pad>', '<eos>', '<bos>', '<unk>'}\n",
    "    s = \" \".join([vocab.get_itos()[token] for token in candidate if vocab.get_itos()[token] not in special_tokens])\n",
    "    print(prefix + f\"序列: {s}\")\n",
    "\n",
    "def generate_text_beam_search(model, input_str, max_len=20, beam_width=5, debug=False):\n",
    "    # model.eval()\n",
    "    input_tokens = [vocab[token] for token in input_str.split()]\n",
    "    # 初始化候选列表\n",
    "    candidates = [(input_tokens, 0.0)]\n",
    "    if debug:\n",
    "        print(len(input_tokens))\n",
    "    if debug:\n",
    "        print_candidate(candidates[0][0], prefix=\"输入\")\n",
    "    with torch.no_grad():\n",
    "        final_results = []\n",
    "        for i in range(max_len): # 最多max_len个token\n",
    "            new_candidates = []\n",
    "            for candidate, candidate_score in candidates:\n",
    "                inputs = torch.LongTensor(candidate).unsqueeze(0).to(device)\n",
    "                # inputs: [1, seq_len]\n",
    "                outputs = model(inputs)\n",
    "                # outputs: [1, seq_len, vocab_size]\n",
    "                logits = outputs[:, -1, :] # 只关心最后一步的数据\n",
    "                # logits [1, vocab_size]\n",
    "                scores, next_tokens = torch.topk(logits, beam_width, dim=-1)\n",
    "                # scores: [1, beam_width]\n",
    "                # next_tokens: [1, beam_width]\n",
    "                for score, next_token in zip(scores.squeeze(), next_tokens.squeeze()):\n",
    "                    new_candidate = candidate + [next_token.item()]\n",
    "                    new_score = candidate_score + score.item()\n",
    "                    if next_token.item() == vocab['<eos>']:\n",
    "                        final_results.append((new_candidate, new_score))\n",
    "                    else:\n",
    "                        new_candidates.append((new_candidate, new_score))\n",
    "            # print(f\"第{i+1}次预测, 共有 {len(new_candidates)} 个候选 {len(final_results)}个结果集\")\n",
    "            # 从新生成的候选中选择最好的 beam_width 个\n",
    "            candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            if debug:\n",
    "                print_candidate(candidates[0][0])\n",
    "            # print(f\"最佳候选序列的token: {[vocab.get_itos()[token] for token in best_candidate]}\")\n",
    "        # 将过程中的遇到<eos>的结果数据也放到候选中\n",
    "        candidates.extend(final_results)\n",
    "        best_candidate, _ = sorted(candidates, key=lambda x: x[1], reverse=True)[0]\n",
    "\n",
    "        special_tokens = {'<pad>', '<eos>', '<bos>', '<unk>'}\n",
    "        best_candidate_strs = [vocab.get_itos()[token] for token in best_candidate if vocab.get_itos()[token] not in special_tokens]\n",
    "        \n",
    "        if debug:\n",
    "            print(len(best_candidate))\n",
    "        return ' '.join(best_candidate_strs)\n",
    "\n",
    "input_str = \"the first\"\n",
    "gen_text = generate_text_beam_search(model, input_str)\n",
    "print(\"input_str\", input_str)\n",
    "print(\"gen_text\", gen_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
