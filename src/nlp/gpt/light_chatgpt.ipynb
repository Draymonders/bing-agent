{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17514\n",
      "2181\n",
      "词汇表大小 65987\n",
      "词汇示例(word2idx) 1707\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from utils import get_tokenizer\n",
    "from vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "data_types = [\"train\", \"test\"]\n",
    "wiki_datas = {}\n",
    "for data_type in data_types:\n",
    "    df = pd.read_csv(f\"{data_type}.csv\")\n",
    "    print(len(df))\n",
    "    df.columns = [\"idx\", \"text\"]\n",
    "    res = []\n",
    "    for i, item in df.iterrows():\n",
    "        text = item.to_dict()[\"text\"]\n",
    "        res.append(text)\n",
    "    wiki_datas[data_type] = res\n",
    "\n",
    "def read_wikitext(data_type):\n",
    "    return wiki_datas[data_type]\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for sentence in data_iter:\n",
    "        # tokenizer 处理一个语句（包含多个单词）\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = read_wikitext(\"train\")\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "vocab.set_default_index(vocab['<pad>'])\n",
    "\n",
    "print(\"词汇表大小\",     len(vocab))\n",
    "print(\"词汇示例(word2idx)\", vocab['am'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65987"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, vocab):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "        self.input_data, self.target_data = self.load_chat_data(file_path)\n",
    "\n",
    "    def load_chat_data(self, file_path):\n",
    "        lines = []\n",
    "        with open(file_path, \"r\") as f:\n",
    "            lines = f.readlines() \n",
    "        input_data, target_data = [], []\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.startswith(\"User:\"):\n",
    "                prefix = \"User: \"\n",
    "                tokens = self.tokenizer(line.strip()[len(prefix):])\n",
    "                tokens = [\"<sos>\"] + tokens + [\"<eos>\"]\n",
    "                idxs = [self.vocab[token] for token in tokens]\n",
    "                input_data.append(torch.tensor(idxs, dtype=torch.long))\n",
    "            elif line.startswith(\"AI:\"):\n",
    "                prefix = \"AI: \"\n",
    "                tokens = self.tokenizer(line.strip()[len(prefix):])\n",
    "                tokens = [\"<sos>\"] + tokens + [\"<eos>\"]\n",
    "                idxs = [self.vocab[token] for token in tokens]\n",
    "                target_data.append(torch.tensor(idxs, dtype=torch.long))\n",
    "\n",
    "        return input_data, target_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_data[idx], self.target_data[idx]\n",
    "\n",
    "    def get_token_strs(self, tokens):\n",
    "        return [self.vocab.get_itos()[token] for token in tokens]\n",
    "        \n",
    "    def item(self, idx):\n",
    "        inp, tgt = self.input_data[idx], self.target_data[idx]\n",
    "        \n",
    "        inp_strs = self.get_token_strs(inp)\n",
    "        tgt_strs = self.get_token_strs(tgt)\n",
    "        inp_s = \" \".join(inp_strs)\n",
    "        tgt_s = \" \".join(tgt_strs)\n",
    "        return inp_s, tgt_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1个case\n",
      "input sentence: <sos> hi , how are you ? <eos>\n",
      "input token: tensor([   1, 9635,    4,  412,   35,  178,  853,    2])\n",
      "input len 8\n",
      "target sentence: <sos> i am doing well , thank you . how about you ? <eos>\n",
      "target data: tensor([    1,    65,  1707,  1616,   120,     4, 14003,   178,     5,   412,\n",
      "           73,   178,   853,     2])\n",
      "target len 14\n",
      "==============================\n",
      "2个case\n",
      "input sentence: <sos> i am good , thanks for asking . what can you do ? <eos>\n",
      "input token: tensor([   1,   65, 1707,  416,    4, 6372,   18, 4148,    5,  185,  112,  178,\n",
      "         283,  853,    2])\n",
      "input len 15\n",
      "target sentence: <sos> i am an ai language model . i can help you answer questions . <eos>\n",
      "target data: tensor([   1,   65, 1707,   31, 2051,  840, 1681,    5,   65,  112,  634,  178,\n",
      "        5949, 4186,    5,    2])\n",
      "target len 16\n",
      "==============================\n",
      "3个case\n",
      "input sentence: <sos> what is the weather like today ? <eos>\n",
      "input token: tensor([   1,  185,   24,    3, 1504,  139,  802,  853,    2])\n",
      "input len 9\n",
      "target sentence: <sos> please check a weather website or application for the current conditions . <eos>\n",
      "target data: tensor([   1, 8961, 6412,   10, 1504, 1748,   48, 3599,   18,    3, 1139, 1406,\n",
      "           5,    2])\n",
      "target len 14\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "file_path = \"chat.txt\"\n",
    "chat_dataset = ChatDataset(file_path, tokenizer, vocab)\n",
    "\n",
    "for i in range(3):\n",
    "    input_s, target_s = chat_dataset.item(i)\n",
    "    input_sample, target_sample = chat_dataset[i]\n",
    "    print(f\"{i+1}个case\")\n",
    "    print(\"input sentence:\", input_s)\n",
    "    print(\"input token:\", input_sample)\n",
    "    print(\"input len\", len(input_sample))\n",
    "    print(\"target sentence:\", target_s)\n",
    "    print(\"target data:\", target_sample)\n",
    "    print(\"target len\", len(target_sample))\n",
    "    print(\"=\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequences, padding_value=0, length=None):\n",
    "    \"\"\"\n",
    "    填充序列，目的sequences token序列长度相同\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences) if length is None else length\n",
    "    # 全零张量\n",
    "    result = torch.full((len(sequences), max_length), padding_value, dtype=torch.long)\n",
    "\n",
    "    for i, seq in enumerate(sequences):\n",
    "        end = len(seq)\n",
    "        result[i, :end] = seq[:end]\n",
    "    return result\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    对batch数据进行预处理，让其src、tgt长度一致\n",
    "    \"\"\"\n",
    "    # batch: [(src1, tgt1), (src2, tgt2), ...]\n",
    "    sources, targets = zip(*batch)\n",
    "\n",
    "    tmps = []\n",
    "    tmps.extend(sources)\n",
    "    tmps.extend(targets)\n",
    "    max_length = max([len(s) for s in tmps])\n",
    "    # print(\"max_length\", max_length)\n",
    "    pad_val = vocab[\"<pad>\"]\n",
    "\n",
    "    sources = pad_sequence(sources, padding_value=pad_val, length=max_length)\n",
    "    targets = pad_sequence(targets, padding_value=pad_val, length=max_length)\n",
    "    return sources, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "chat_dataloader = DataLoader(chat_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (decoder): Decoder(\n",
      "    (src_emb): Embedding(65987, 512)\n",
      "    (pos_emb): Embedding(256, 512)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x DecoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): PoswiseFeedForwardNet(\n",
      "          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
      "          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projection): Linear(in_features=512, out_features=65987, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from gpt_model import GPT\n",
    "voc_size = len(vocab)\n",
    "max_seq_len = 256\n",
    "n_layers = 6\n",
    "\n",
    "model = GPT(voc_size, max_seq_len, n_layers)\n",
    "model.load_state_dict(torch.load(\"wikitext_best.pth\", map_location=torch.device(\"cpu\")))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0020, cost = 0.263370\n",
      "Epoch: 0040, cost = 0.020510\n",
      "Epoch: 0060, cost = 0.003986\n",
      "Epoch: 0080, cost = 0.557733\n",
      "Epoch: 0100, cost = 0.090689\n",
      "Epoch: 0120, cost = 0.006321\n",
      "Epoch: 0140, cost = 0.071676\n",
      "Epoch: 0160, cost = 0.024505\n",
      "Epoch: 0180, cost = 0.002932\n",
      "Epoch: 0200, cost = 0.024535\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # 损失函数\n",
    "\n",
    "def freeze_layers(model, n):\n",
    "    params_to_update = [] # 参数\n",
    "    for name, param in model.named_parameters():\n",
    "        # print(name) # debug 看下参数名\n",
    "        # try:\n",
    "        #     if int(name.split(\".\")[2]) >= n:\n",
    "        #         params_to_update.append(param)\n",
    "        # except:\n",
    "        #     pass\n",
    "        params_to_update.append(param)\n",
    "    return params_to_update\n",
    "\n",
    "params_to_update = freeze_layers(model, n=2) # 冻结前两层\n",
    "# print(len(params_to_update))\n",
    "optimizer = optim.Adam(params_to_update, lr=learning_rate)\n",
    "\n",
    "min_loss = float(\"inf\")\n",
    "save_path = \"light_chatgpt_best.pth\"\n",
    "\n",
    "for epoch in range(200): # 开始训练\n",
    "    for batch_idx, (input_batch, target_batch) in enumerate(chat_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        # print(input_batch.shape)\n",
    "        # print(target_batch.shape)\n",
    "        # print(input_batch)\n",
    "        # print(target_batch)\n",
    "        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "        outputs = model(input_batch)\n",
    "        # print(outputs.view(-1, len(vocab)).shape)\n",
    "        # print(target_batch.view(-1).shape)\n",
    "        loss = loss_fn(outputs.view(-1, len(vocab)), target_batch.view(-1))\n",
    "        single_loss = loss.item() / len(input_batch)\n",
    "        if single_loss < min_loss:\n",
    "            min_loss = single_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch: {epoch + 1:04d}, cost = {loss:6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_str what is the weather like today ?\n",
      "gen_text what is the weather like today ? application weather weather weather weather weather weather weather weather weather weather weather weather weather weather weather weather weather weather weather\n",
      "==============================\n",
      "input_str hi , how are you ?\n",
      "gen_text hi , how are you ? thank you , am am am am am am am am am am am am am am am am am\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# 集束搜索\n",
    "def print_candidate(candidate, prefix=\"\"):\n",
    "    special_tokens = {'<pad>', '<eos>', '<bos>', '<unk>'}\n",
    "    s = \" \".join([vocab.get_itos()[token] for token in candidate if vocab.get_itos()[token] not in special_tokens])\n",
    "    print(prefix + f\"序列: {s}\")\n",
    "\n",
    "def generate_text_beam_search(model, input_str, max_len=20, beam_width=5, debug=False):\n",
    "    # model.eval()\n",
    "    input_tokens = [vocab[token] for token in input_str.split()]\n",
    "    # 初始化候选列表\n",
    "    candidates = [(input_tokens, 0.0)]\n",
    "    if debug:\n",
    "        print(len(input_tokens))\n",
    "    if debug:\n",
    "        print_candidate(candidates[0][0], prefix=\"输入\")\n",
    "    with torch.no_grad():\n",
    "        final_results = []\n",
    "        for i in range(max_len): # 最多max_len个token\n",
    "            new_candidates = []\n",
    "            for candidate, candidate_score in candidates:\n",
    "                inputs = torch.LongTensor(candidate).unsqueeze(0).to(device)\n",
    "                # inputs: [1, seq_len]\n",
    "                outputs = model(inputs)\n",
    "                # outputs: [1, seq_len, vocab_size]\n",
    "                logits = outputs[:, -1, :] # 只关心最后一步的数据\n",
    "                # logits [1, vocab_size]\n",
    "                scores, next_tokens = torch.topk(logits, beam_width, dim=-1)\n",
    "                # scores: [1, beam_width]\n",
    "                # next_tokens: [1, beam_width]\n",
    "                for score, next_token in zip(scores.squeeze(), next_tokens.squeeze()):\n",
    "                    new_candidate = candidate + [next_token.item()]\n",
    "                    new_score = candidate_score + score.item()\n",
    "                    if next_token.item() == vocab['<eos>']:\n",
    "                        final_results.append((new_candidate, new_score))\n",
    "                    else:\n",
    "                        new_candidates.append((new_candidate, new_score))\n",
    "            # print(f\"第{i+1}次预测, 共有 {len(new_candidates)} 个候选 {len(final_results)}个结果集\")\n",
    "            # 从新生成的候选中选择最好的 beam_width 个\n",
    "            candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            if debug:\n",
    "                print_candidate(candidates[0][0])\n",
    "            # print(f\"最佳候选序列的token: {[vocab.get_itos()[token] for token in best_candidate]}\")\n",
    "        # 将过程中的遇到<eos>的结果数据也放到候选中\n",
    "        candidates.extend(final_results)\n",
    "        best_candidate, _ = sorted(candidates, key=lambda x: x[1], reverse=True)[0]\n",
    "\n",
    "        special_tokens = {'<pad>', '<eos>', '<bos>', '<unk>'}\n",
    "        best_candidate_strs = [vocab.get_itos()[token] for token in best_candidate if vocab.get_itos()[token] not in special_tokens]\n",
    "        \n",
    "        if debug:\n",
    "            print(len(best_candidate))\n",
    "        return ' '.join(best_candidate_strs)\n",
    "\n",
    "\n",
    "input_strs = [\"what is the weather like today ?\", \"hi , how are you ?\"]\n",
    "for inp_s in input_strs:\n",
    "    gen_text = generate_text_beam_search(model, inp_s)\n",
    "    print(\"input_str\", inp_s)\n",
    "    print(\"gen_text\", gen_text)\n",
    "    print(\"=\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
