{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "d_k = 64 # Q的维度\n",
    "d_v = 64 # V的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    缩放点积注意力\n",
    "    简单理解 ScaledDotProductAttention，目的是计算Query和Key的相似权重，作用于Value\n",
    "    结果是\n",
    "    Query1: {Value1: w11, Value2: w12, Value3: w13}\n",
    "    Query2: {Value1: w21, Value2: w22, Value3: w23}\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # 维度信息\n",
    "        # Q: [batch_size, n_heads, len_q, d_k]\n",
    "        # K: [batch_size, n_heads, len_k, d_k]\n",
    "        # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "        # attn_mask: [batch_size, n_heads, len_q, len_k]\n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size, n_heads, len_q, len_k]\n",
    "        # scores: [batch_size, n_heads, len_q, len_k]\n",
    "        # 加上注意力掩码, 将attn_mask中为True的位置的分数设置为极小值\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is True.\n",
    "        # softmax归一化 => 注意力权重\n",
    "        weights = nn.Softmax(dim=-1)(scores)\n",
    "        # weights: [batch_size, n_heads, len_q, len_k]\n",
    "        context = torch.matmul(weights, V) \n",
    "        # context: [batch_size, n_heads, len_q, d_v]\n",
    "        return context, weights # 返回上下文变量 和 注意力分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embedding = 512 # embedding的维度\n",
    "n_heads = 8 # 多头注意力的个数\n",
    "batch_size = 3\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    多头注意力\n",
    "    简单理解，先放大维度，提取Q、K、V的各个维度的信息，再缩小维度，得到最终的结果\n",
    "    黑盒的看是 (Q、K、V) -> Q\n",
    "    \"\"\"\n",
    "    def __init__(self, d_embedding=d_embedding, n_heads=n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_embedding = d_embedding\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(d_embedding, n_heads * d_k)\n",
    "        self.W_K = nn.Linear(d_embedding, n_heads * d_k)\n",
    "        self.W_V = nn.Linear(d_embedding, n_heads * d_v)\n",
    "        self.linear = nn.Linear(n_heads * d_v, d_embedding)\n",
    "        self.layer_norm = nn.LayerNorm(d_embedding)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # 维度信息\n",
    "        # Q: [batch_size, len_q, d_embedding]\n",
    "        # K: [batch_size, len_k, d_embedding]\n",
    "        # V: [batch_size, len_v(=len_k), d_embedding]\n",
    "        # attn_mask: [batch_size, len_q, len_k]\n",
    "        \n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # 线性层，维度提升，为了捕捉更多信息\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2) \n",
    "        # q_s: [batch_size, n_heads, len_q, d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)\n",
    "        # k_s: [batch_size, n_heads, len_k, d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)\n",
    "        # v_s: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "        # attn_mask: [batch_size, n_heads, len_q, len_k]\n",
    "\n",
    "        # 点积缩放注意力\n",
    "        context, weights = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        # context: [batch_size, n_heads, len_q, d_v]\n",
    "        # weights: [batch_size, n_heads, len_q, len_k]\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)\n",
    "        # context: [batch_size, len_q, n_heads * d_v]\n",
    "\n",
    "        # 线性层，降维成 Q 原始的维度\n",
    "        output = self.linear(context) \n",
    "        # output: [batch_size, len_q, d_embedding]\n",
    "        \n",
    "        # 残差连接，并做归一化（方便将当前Q往下层传递，所以做了残差）\n",
    "        output = self.layer_norm(output + residual) \n",
    "        # output: [batch_size, len_q, d_embedding]\n",
    "        return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    \"\"\"\n",
    "    前馈神经网络，目标是优化每个标记（单词）的表征\n",
    "    对每个位置的d_embedding维度进行升维 => 降维 => 残差归一化\n",
    "    \"\"\"\n",
    "    def __init__(self, d_ff=2048):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        # 输入升维\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_embedding, out_channels=d_ff, kernel_size=1)\n",
    "        # 输入降维\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_embedding, kernel_size=1)\n",
    "        # 定义 归一化\n",
    "        self.layer_norm = nn.LayerNorm(d_embedding)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs [batch_size, len_q, d_embedding]\n",
    "        residual = inputs\n",
    "\n",
    "        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n",
    "        # [batch_size, d_ff, len_q]\n",
    "\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        # [batch_size, len_q, d_embedding]\n",
    "        \n",
    "        output = self.layer_norm(output + residual)\n",
    "        # [batch_size, len_q, d_embedding]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_enc_table(n_position, embedding_dim):\n",
    "    # 位置编码表：目的是让模型知道输入序列中单词的位置信息\n",
    "    # 也可以用自然序列(1,2,3)作为位置编码，但正余弦能更好表达位置信息\n",
    "    # 维度信息\n",
    "    # n_position: 输入序列最大长度\n",
    "    # embedding_dim: 词向量维度\n",
    "\n",
    "    pos_table = np.zeros((n_position, embedding_dim), dtype=np.float32)\n",
    "    for pos_i in range(n_position):\n",
    "        for idx in range(embedding_dim):\n",
    "            angle = pos_i / np.power(10000, 2 * (idx // 2) / embedding_dim)\n",
    "            pos_table[pos_i, idx] = angle\n",
    "    \n",
    "    pos_table[:, 0::2] = np.sin(pos_table[:, 0::2]) # dim 2i偶数维\n",
    "    pos_table[:, 1::2] = np.cos(pos_table[:,1::2]) # dim 2i+1奇数维\n",
    "    # pos_table: [n_position, embedding_dim]\n",
    "    return torch.FloatTensor(pos_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    # 填充注意力掩码\n",
    "    # seq_q: [batch_size, len_q]\n",
    "    # seq_k: [batch_size, len_k]\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "\n",
    "    # =0的位置会变成True,其他是False\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1) \n",
    "    # [batch_size, 1, len_k]\n",
    "\n",
    "    pad_aatn_mask = pad_attn_mask.expand(batch_size, len_q, len_k)\n",
    "    # [batch_size, len_q, len_k]\n",
    "    return pad_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention() # 多头注意力\n",
    "        self.pos_ffn = PoswiseFeedForwardNet() # 逐位前馈网络\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        # enc_inputs: [batch_size, seq_len, embedding_dim]\n",
    "        # enc_self_attn_mask: [batch_size, seq_len, seq_len]\n",
    "\n",
    "        # Q、K、V都是本身\n",
    "        enc_outputs, attn_weights = self.enc_self_attn(\n",
    "            enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "\n",
    "        # enc_outputs: [batch_size, seq_len, embedding_dim]\n",
    "        # attn_weights: [batch_size, n_heads, seq_len, seq_len]\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) \n",
    "        # enc_outputs: [batch_size, seq_len, embedding_dim]\n",
    "        return enc_outputs, attn_weights  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 2 # Encoder层数\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, corpus, n_layers=6, drop_p=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_emb = nn.Embedding(len(corpus.src_vocab), d_embedding)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(\n",
    "            get_pos_enc_table(corpus.src_len+1, d_embedding), freeze=True)\n",
    "        # freeze=True, 位置编码固定，不用更新\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer() for _ in range(n_layers)\n",
    "        ])\n",
    "        # self.dropout = nn.Dropout(drop_p)\n",
    "    \n",
    "    def forward(self, enc_inputs):\n",
    "        # enc_inputs: [batch_size, seq_len]\n",
    "\n",
    "        pos_idxs = torch.arange(1, enc_inputs.size(1) + 1).unsqueeze(0).to(enc_inputs)\n",
    "        # pos_idx [1, seq_len]\n",
    "\n",
    "        # embedding(词) + embedding(位置)\n",
    "        enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(pos_idxs)\n",
    "        # enc_outpus [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # 忽略下某些信息\n",
    "        # enc_outputs = self.dropout(enc_outputs)\n",
    "        # enc_outpus [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "        # 获取掩码\n",
    "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
    "\n",
    "        # 多层Encoder Layer\n",
    "        enc_self_attn_weights = []\n",
    "        for layer in self.layers:\n",
    "            enc_outputs, enc_self_attn_weight = layer(\n",
    "                enc_outputs, enc_self_attn_mask)\n",
    "            enc_self_attn_weights.append(enc_self_attn_weight)\n",
    "\n",
    "        # enc_outputs = self.dropout(enc_outputs)\n",
    "        # enc_outputs: [batch_size, seq_len, embedding_dim],\n",
    "        # enc_self_attn_mask: list[batch_size, n_head, seq_len, seq_len]\n",
    "        return  enc_outputs, enc_self_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_subsequent_mask(seq):\n",
    "    # seq: [batch_size, seq_len(Q)=seq_len(K)\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    # attn_shape: [batch_size, seq_len, seq_len]\n",
    "\n",
    "    # triu triangle upper\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
    "    # subsequent_mask: [batch_size, seq_len, seq_len]\n",
    "\n",
    "    subsequent_mask = torch.from_numpy(subsequent_mask).byte()\n",
    "    # subsequent_mask: [batch_size, seq_len, seq_len]\n",
    "    return subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        self.dec_enc_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
    "        # dec_inputs [batch_size, tgt_len, d_model]\n",
    "        # enc_outputs [batch_size, src_len, d_model]\n",
    "        # dec_self_attn_mask [batch_size, tgt_len, tgt_len]\n",
    "        # dec_enc_attn_mask [batch_size, tgt_len, src_len]\n",
    "\n",
    "\n",
    "        # 先自注意自己\n",
    "        dec_outputs, dec_self_attn = self.dec_self_attn(\n",
    "            dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
    "        # dec_outputs [batch_size, tgt_len, d_model]\n",
    "        # dec_self_attn [batch_size, n_heads, tgt_len, tgt_len]\n",
    "\n",
    "        # 再注意Encoder的隐藏层\n",
    "        dec_outputs, dec_enc_attn = self.dec_enc_attn(\n",
    "            dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        # dec_outputs [batch_size, tgt_len, d_model]\n",
    "        # dec_enc_attn [batch_size, h_heads, tgt_len, src_len]\n",
    "\n",
    "        dec_outpus = self.pos_ffn(dec_outputs)\n",
    "        return dec_outputs, dec_self_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, corpus):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tgt_emb = nn.Embedding(len(corpus.tgt_vocab), d_embedding)\n",
    "        \n",
    "        self.pos_emb = nn.Embedding.from_pretrained(\n",
    "            get_pos_enc_table(corpus.tgt_len+1, d_embedding), freeze=True)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
    "        # dec_inputs: [batch_size, tgt_len]\n",
    "        # enc_inputs: [batch_size, src_len]\n",
    "        # enc_outputs: [batch_size, src_len, embedding_dim]\n",
    "\n",
    "        # 位置索引\n",
    "        pos_idxs = torch.arange(1, dec_inputs.size(1)+1).unsqueeze(0).to(dec_inputs)\n",
    "        # pos_idxs: [1, tgt_len]\n",
    "        dec_outputs = self.tgt_emb(dec_inputs) + self.pos_emb(pos_idxs)\n",
    "        # dec_outputs: [batch_size, tgt_len, embedding_dim]\n",
    "        \n",
    "        # 解码自注意力掩码\n",
    "        # 位置掩码（长度不够的序列补<pad>）\n",
    "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs)\n",
    "        # 后续掩码（训练过程中不能看到后续的token）\n",
    "        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs)\n",
    "        # 合并 位置掩码，后续掩码\n",
    "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n",
    "        # dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\n",
    "\n",
    "        # 解码-编码注意力掩码\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
    "        # dec_self_attn_mask: [batch_size, tgt_len, src_len]\n",
    "\n",
    "        dec_self_attns, dec_enc_attns = [], []\n",
    "        for layer in self.layers:\n",
    "            dec_outputs, dec_self_attn, dec_enc_attn = layer(\n",
    "                dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            # dec_outputs: [batch_size, tgt_len, embedding_dim]\n",
    "            # dec_self_attn: [batch_size, tgt_len, tgt_len]\n",
    "            # dec_enc_attn: [batch_size, tgt_len, src_len]\n",
    "            dec_self_attns.append(dec_self_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return dec_outputs, dec_self_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, corpus):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.corpus = corpus\n",
    "        self.encoder = Encoder(corpus)\n",
    "        self.decoder = Decoder(corpus)\n",
    "        # 将解码器输出转换为目标词汇表的概率分布\n",
    "        self.projection = nn.Linear(d_embedding, len(corpus.tgt_vocab), bias=False)\n",
    "\n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        # enc_inputs: [batch_size, src_len]\n",
    "        # dec_inputs: [batch_size, tgt_len]\n",
    "\n",
    "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
    "        # enc_outputs: [batch_size, src_len, d_embedding]\n",
    "        # enc_self_attns: list([batch_size, n_heads, src_len, src_len])\n",
    "\n",
    "\n",
    "        # encoder_input用来和decoder_output做掩码的，即 encoder里面短的序列，decoder不关注超过短序列的信息\n",
    "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        # dec_outputs: [batch_size, tgt_len, d_embedding]\n",
    "        # dec_self_attns: list([batch_size, n_heads, tgt_len, tgt_len])\n",
    "        # dec_enc_attns: list([batch_size, n_heads, tgt_len, src_len])\n",
    "        \n",
    "        # 解码器的输出通过一个全连接层得到最终的输出\n",
    "        dec_logits = self.projection(dec_outputs)\n",
    "        return dec_logits, enc_self_attns, dec_self_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class TranslationCorpus:\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "        \n",
    "        # +1是容纳<pad>\n",
    "        self.src_len = max(len(sentence[0].split()) for sentence in sentences) + 1\n",
    "        # +2是容纳<sos>和<eos>\n",
    "        self.tgt_len = max(len(sentence[1].split()) for sentence in sentences) + 3\n",
    "\n",
    "        self.src_vocab, self.tgt_vocab = self.create_vocab()\n",
    "\n",
    "        self.src_idx2word = {v: k for k, v in self.src_vocab.items()}\n",
    "        self.tgt_idx2word = {v: k for k, v in self.tgt_vocab.items()}\n",
    "\n",
    "    def create_vocab(self):\n",
    "        src_counter = Counter(word for sentence in self.sentences for word in sentence[0].split())\n",
    "        tgt_counter = Counter(word for sentence in self.sentences for word in sentence[1].split())\n",
    "        \n",
    "        src_vocab = {\n",
    "            \"<pad>\": 0,\n",
    "            \"<unknown>\": 1\n",
    "            # **{word: i+1 ,\n",
    "        }\n",
    "        src_unknown_idx = 1\n",
    "        word_max_cnt = 50\n",
    "        for i, word in enumerate(src_counter):\n",
    "            # if src_counter[word] >= word_max_cnt:\n",
    "            #     # src_vocab[word] = src_unknown_idx \n",
    "            #     continue\n",
    "            src_vocab[word] = i + 2\n",
    "        \n",
    "\n",
    "        tgt_vocab = {\n",
    "            \"<pad>\": 0,\n",
    "            \"<sos>\": 1,\n",
    "            \"<eos>\": 2,\n",
    "            \"<unkonwn>\": 3,\n",
    "        }\n",
    "        for i, word in enumerate(tgt_counter):\n",
    "            # if tgt_counter[word] >= word_max_cnt:\n",
    "            #     continue\n",
    "            tgt_vocab[word] = i+4\n",
    "        return src_vocab, tgt_vocab\n",
    "\n",
    "    def make_batch(self, batch_size, test_batch=False):\n",
    "        input_batch, output_batch, target_batch = [], [], []\n",
    "        batch_size = min(batch_size, len(self.sentences))\n",
    "        sentence_idxs = torch.randperm(len(self.sentences))[:batch_size]\n",
    "        for idx in sentence_idxs:\n",
    "            src_sentence, tgt_sentence = self.sentences[idx]\n",
    "            src_seq = [self.src_vocab[word] for word in src_sentence.split() if word in self.src_vocab]\n",
    "            tgt_seq = [self.tgt_vocab['<sos>']] + [self.tgt_vocab[word] for word in tgt_sentence.split() if word in self.tgt_vocab ]  + [self.tgt_vocab['<eos>']]\n",
    "            # 数据填充\n",
    "            src_seq += [self.src_vocab['<pad>']] * (self.src_len - len(src_seq))\n",
    "            tgt_seq += [self.tgt_vocab['<pad>']] * (self.tgt_len - len(tgt_seq))\n",
    "            input_batch.append(src_seq)\n",
    "            if test_batch:\n",
    "                # 验证阶段，输出序列初始值为[\"sos\", \"pad\", \"pad\"]\n",
    "                output_batch.append(\n",
    "                    [self.tgt_vocab['<sos>']] + ([self.tgt_vocab['<pad>']] * (self.tgt_len-2))\n",
    "                )\n",
    "            else:\n",
    "                output_batch.append(tgt_seq[:-1])\n",
    "            target_batch.append(tgt_seq[1:])\n",
    "        return torch.LongTensor(input_batch), torch.LongTensor(output_batch), torch.LongTensor(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [\"咖哥 很 喜欢 小冰\", \"KaGe likes XiaoBing much\"],\n",
    "    [\"我 爱 学习 人工智能\", \"I love studying AI\"],\n",
    "    [\"深度学习 改变 世界\", \"DL changed the world\"],\n",
    "    [\"自然语言 处理 很 强大\", \"NLP is so powerful\"],\n",
    "    [\"神经网络 非常 复杂\", \"Neural Nets are complex\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2191\n",
      "示例语句 ['< 上市 公司 重大 资产 重组 管理 办法 > 已经 2008 年 3月 24日 中国 证券 监督 管理 委员会 第224 次 主席 办公会议 审议 通过 , 现 予 公布 , 自 2008 年 5月 18日 起 施行 .', 'The Measures for Administration of Material Assets Reorganization of Listed Companies, which were adopted at the 224th President’s Meeting of China Securities Regulatory Commission on March 24, 2008, are hereby promulgated and shall come into force as of May 18, 2008.']\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "dataset_prefix = \"SPC.en-zh.\"\n",
    "source = \"zh\"\n",
    "target = \"en\"\n",
    "pairs = [source, target]\n",
    "\n",
    "datas = {\n",
    "    \"zh\": [],\n",
    "    \"en\": []\n",
    "}\n",
    "for k in pairs:\n",
    "    with open(f\"{dataset_prefix}{k}\", \"r\") as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if len(line) == 0:\n",
    "                break\n",
    "            datas[k] += [line]\n",
    "for idx in range(len(datas[\"zh\"])):\n",
    "    cn = datas[\"zh\"][idx].strip()\n",
    "    en = datas[\"en\"][idx].strip()\n",
    "    cn_words = cn.split()\n",
    "    en_words = en.split()\n",
    "    # if len(cn_words) > 10 or len(cn_words) < 3:\n",
    "    if len(cn_words) < 3:\n",
    "        continue\n",
    "    if len(en_words) > 100 or len(en_words) < 3:\n",
    "    # if len(en_words) > 20 or len(en_words) < 3:\n",
    "        continue\n",
    "    sentences += [[cn, en]]\n",
    "print(len(sentences))\n",
    "print(\"示例语句\", sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src词表大小 3565\n",
      "tgt词表大小 6423\n"
     ]
    }
   ],
   "source": [
    "corpus = TranslationCorpus(sentences)\n",
    "print(\"src词表大小\", len(corpus.src_vocab))\n",
    "print(\"tgt词表大小\", len(corpus.tgt_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  010 cost=2.387147\n",
      "epoch:  020 cost=1.252282\n",
      "epoch:  030 cost=1.730028\n",
      "epoch:  040 cost=3.247510\n",
      "epoch:  050 cost=1.088288\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "model = Transformer(corpus)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "learning_rate = 0.0001 \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epoches = 50 # 迭代轮次\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    enc_inputs, dec_inputs, target_batch = corpus.make_batch(batch_size)\n",
    "    outputs, _, _, _ = model(enc_inputs, dec_inputs)\n",
    "    loss = loss_fn(outputs.view(-1, len(corpus.tgt_vocab)), target_batch.view(-1))\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"epoch: {epoch+1: 04d} cost={loss:6f}\")\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_batch():\n",
    "    enc_inputs, dec_inputs, target_batch = corpus.make_batch(batch_size=1, test_batch=True)\n",
    "    predict, _, _, _ = model(enc_inputs, dec_inputs)\n",
    "    predict = predict.view(-1, len(corpus.tgt_vocab))\n",
    "    predict = predict.data.max(1, keepdim=True)[1] # 找到每个位置概率最大单词的索引\n",
    "\n",
    "    translated_sentence = [corpus.tgt_idx2word[idx.item()] for idx in predict.squeeze()]\n",
    "    input_sentence = [corpus.src_idx2word[idx.item()] for idx in enc_inputs[0]]\n",
    "\n",
    "    def list_to_str(vals=[], sep=\" \"):\n",
    "        return sep.join(vals)\n",
    "    print(\"input: \", list_to_str(input_sentence))\n",
    "    print(\"translate: \", list_to_str(translated_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  本次 重组 的 重大 资产 重组 报告书 , 独立 财务 顾问 报告 , 法律 意见书 以及 重组 涉及 的 审计 报告 , 资产 评估 报告 和 经 审核 的 盈利 预测 报告 最迟 应当 与 召开 股东 大会 的 通知 同时 公告 . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "translate:  the <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "input:  保险 公司 应当 在 合规 管理 部门 与 内部 审计 部门 之间 建立 明确 的 合作 和 信息 交流 机制 。 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "translate:  the <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "input:  （ 一 ） 变更 名称 ； <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "translate:  the <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "input:  证券 公司 停止 全部 证券 业务 , 解散 , 破产 或者 撤销 境内 分支 机构 的 , 应当 在 国务院 证券 监督 管理 机构 指定 的 报刊 上 公告 , 并 按照 规定 将 经营 证券 业务 许可证 交 国务院 证券 监督 管理 机构 注销 . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "translate:  the <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "input:  国务院 关税 税则 委员会 规定 按 货物 征税 的 进境 物品 ， 按照 本 条例 第二 章 至 第四 章 的 规定 征收 关税 。 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "translate:  the <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    test_single_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
